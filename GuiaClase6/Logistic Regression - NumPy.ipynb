{"cells":[{"cell_type":"markdown","metadata":{"id":"U6cW6Tt71ik7"},"source":["# Logistic Regression - NumPy"]},{"cell_type":"markdown","metadata":{"id":"ow8ElyEq1ilG"},"source":["El objetivo de éste ejercicio es que implementen paso a paso los building blocks del modelo de regresión logística, para finalmente crear una clase del modelo."]},{"cell_type":"markdown","metadata":{"id":"0-OtSR4S1ilI"},"source":["## Cargamos las Librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GelzAxhl1ilK"},"outputs":[],"source":["import numpy as np\n","import time\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"5c1yUsFQ1ilM"},"source":["## Implementación de Building Blocks del Modelo"]},{"cell_type":"markdown","metadata":{"id":"gH4ZrYLq1ilN"},"source":["A continuación, se deberán implementar paso a paso los distintos bloques de código que conforman el modelo, junto con algunas funciones auxiliares."]},{"cell_type":"markdown","metadata":{"id":"3cQ5HGjo1ilO"},"source":["### Función Sigmoid"]},{"cell_type":"markdown","metadata":{"id":"IKEkY5Mu1ilQ"},"source":["Implementar la función: $g(z) = \\frac{1}{1 + e^{-z}}$ en NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYVpVyGC1ilR"},"outputs":[],"source":["# TODO\n","def sigmoid(x):\n","  return 1/(1 + np.exp(-x))"]},{"cell_type":"markdown","metadata":{"id":"QqEUZVI71ilT"},"source":["### Binary Cross Entropy"]},{"cell_type":"markdown","metadata":{"id":"-GidRCnI1ilV"},"source":["Implementar la función de costo: $J(w) = \\frac{1}{n}\\sum_{i=1}^{n}L\\left ( \\hat{y},y \\right )= \\frac{1}{n}\\sum_{i=1}^{n}\\left [y^{(i)}log(\\hat{y}^{(i)})+ (1-y^{(i)})log(1-\\hat{y}^{(i)}) \\right ]$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHlcXFNo1ilX"},"outputs":[],"source":["# TODO\n","def loss(y_true, y_pred):\n","  return np.mean(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))"]},{"cell_type":"markdown","metadata":{"id":"v2RAJs641ilY"},"source":["### Gradiente"]},{"cell_type":"markdown","metadata":{"id":"c0SOON501ilZ"},"source":["Implementar el gradiente de la función costo respecto de los parámetros: $\\frac{\\partial J(w)}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^{n}\\left ( \\hat{y}^{i}-y^{i}\\right )\\bar{x}^i$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZaUg5z_1ila"},"outputs":[],"source":["# TODO\n","def grad_loss(y_true, y_pred, x):\n","  return np.mean((y_pred-y_true)*x)"]},{"cell_type":"markdown","metadata":{"id":"pfARfXhh1ilb"},"source":["### Normalización"]},{"cell_type":"markdown","metadata":{"id":"hqeHqr_O1ilb"},"source":["Implementar normalización Z-score de las features de entrada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUI3pmp-1ilc"},"outputs":[],"source":["# TODO\n","def z_score(x):\n","  return (x-np.mean(x))/np.var(x)"]},{"cell_type":"markdown","metadata":{"id":"dLQerv2U1ilc"},"source":["### Métricas (Precision, Recall y Accuracy)"]},{"cell_type":"markdown","metadata":{"id":"dA3vrBcv1ild"},"source":["Implementar las métricas en NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYQEugfr1ile"},"outputs":[],"source":["# TODO\n","def metricas(y_pred, y_true):\n","  Tr = np.array(x)\n","  Pr = np.array(y)\n","  TP = np.size(np.where(Pr+Tr==0)) #sum(Tr & Pr)\n","  TN = np.size(np.where(Pr+Tr==2))\n","  FN = np.size(np.intersect1d(np.where(Pr+Tr==1),np.where(Tr)))\n","  FP = np.size(np.intersect1d(np.where(Pr+Tr==1),np.where(Pr)))\n","    \n","  P = TP/(TP+FP) #Precision\n","  R = TP/(TP+FN) #Recall\n","  A = (TP+TN)/(TP+TN+FN+FP) #Accuracy\n","    \n","  return P, R, A"]},{"cell_type":"markdown","metadata":{"id":"dsf3-jQM1ile"},"source":["### Implementar función fit"]},{"cell_type":"markdown","metadata":{"id":"xhnP6Uvc1ilf"},"source":["Utilizas los bloques anteriores, junto con la implementación en NumPy del algoritmo Mini-Batch gradient descent, para crear la función fit de nuestro modelo de regresión logística. Cada un determinado número de epochs calculen el loss, almacénenlo en una lista y hagan un log de los valores. La función debe devolver los parámetros ajustados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6syf61c1ilf"},"outputs":[],"source":["# TODO\n","def fit(self, X, y, lr, b, epochs, bias=True):\n","    # si decidimos utilizar bias, agregamos como siempre una columna con '1' al dataset de entrada\n","    if bias:\n","        X = np.hstack((np.ones((X.shape[0], 1)), X))\n","\n","    # inicializamos aleatoriamente los pesos\n","    w = np.random.rand(len(X))\n","    \n","    loss_list = []\n","\n","    # corremos Mini-Batch para optimizar los parámetros\n","    for j in range(epochs):\n","        idx = np.random.permutation(X.shape[0])\n","        X_train = X[idx]\n","        y_train = y[idx]\n","        batch_size = int(len(X_train)/b)\n","\n","        for i in range(0, len(X_train), batch_size):\n","            # Seleccionar los elementos del batch actual\n","            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n","            batch_X = X_train[i: end]\n","            batch_y = y_train[i: end]\n","\n","            # cálculo de predicciones\n","            z = np.dot(batch_X, W) \n","            prediction = self.sigomid(z) #Llamada a la funcion sigmoidea\n","            # cálculo del error\n","            error = prediction.reshape(-1, 1) - batch_y.reshape(-1, 1)\n","            # cálculo del grandiente\n","            grad_sum = np.sum(error*batch_X, axis=0)\n","            grad_mul = 1/batch_size*grad_sum\n","            gradient = np.transpose(grad_mul).reshape(-1, 1)\n","            #actualizar pesos\n","            W = W  - lr*gradient\n","\n","        z = np.dot(X_train, W)\n","        l_epoch = self.loss(y_train, self.sigmoid(z))\n","        loss_list.append(l_epoch)\n","    \n","    self.model = W\n","    self.losses = loss_list\n","    return loss_list, W"]},{"cell_type":"markdown","metadata":{"id":"GO6A0wsV1ilh"},"source":["### Implementar función predict"]},{"cell_type":"markdown","metadata":{"id":"wUa2g6S91ilh"},"source":["Implementar la función predict usando los parámetros calculados y la función sigmoid. Prestar atención a las transformaciones de los datos de entrada. Asimismo, se debe tomar una decisión respecto de los valores de salida como: $p\\geq 0.5 \\to 1, p<0.5 \\to 0$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08WKPBHQ1ili"},"outputs":[],"source":["# TODO\n","def predict(self, X):\n","    X = np.hstack((np.ones((X.shape[0], 1)), X))\n","    z = np.dot(X, self.model)\n","    p = self.sigmoid(z)\n","    mask_true = p >= 0.5\n","    mask_false = p < 0.5\n","    p[mask_true] = 1\n","    p[mask_false] = 0\n","    return p"]},{"cell_type":"markdown","metadata":{"id":"k1294bnv1ili"},"source":["## Armar una clase LogisticRegression"]},{"cell_type":"markdown","source":["Clase BaseModel"],"metadata":{"id":"Iyv9xrf_GCSi"}},{"cell_type":"code","source":["class BaseModel(object):\n","\n","    def __init__(self):\n","        self.model = None\n","\n","    def fit(self, X, Y):\n","        return NotImplemented\n","\n","    def predict(self, X):\n","        return NotImplemented"],"metadata":{"id":"1o-lytUWErIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H6-GHsqN1ilj"},"source":["Armar una clase LogisticRegression que herede de BaseModel y tenga la siguiente estructura:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z03pQKSu1ilk"},"outputs":[],"source":["class LogisticRegression(BaseModel):\n","    \n","    def sigmoid(self, x):\n","        return 1/(1 + np.exp(-x))\n","\n","    def loss(self, y_true, y_pred):\n","        return np.mean(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n","\n","    def fit(self, X, y, lr, b, epochs, bias=True):\n","        # si decidimos utilizar bias, agregamos como siempre una columna con '1' al dataset de entrada\n","        if bias:\n","            X = np.hstack((np.ones((X.shape[0], 1)), X))\n","\n","        # inicializamos aleatoriamente los pesos\n","        w = np.random.rand(len(X))\n","        \n","        loss_list = []\n","\n","        # corremos Mini-Batch para optimizar los parámetros\n","        for j in range(epochs):\n","            idx = np.random.permutation(X.shape[0])\n","            X_train = X[idx]\n","            y_train = y[idx]\n","            batch_size = int(len(X_train)/b)\n","\n","            for i in range(0, len(X_train), batch_size):\n","                # Seleccionar los elementos del batch actual\n","                end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n","                batch_X = X_train[i: end]\n","                batch_y = y_train[i: end]\n","\n","                # cálculo de predicciones\n","                z = np.dot(batch_X, W) \n","                prediction = self.sigomid(z) #Llamada a la funcion sigmoidea\n","                # cálculo del error\n","                error = prediction.reshape(-1, 1) - batch_y.reshape(-1, 1)\n","                # cálculo del grandiente\n","                grad_sum = np.sum(error*batch_X, axis=0)\n","                grad_mul = 1/batch_size*grad_sum\n","                gradient = np.transpose(grad_mul).reshape(-1, 1)\n","                #actualizar pesos\n","                W = W  - lr*gradient\n","\n","            z = np.dot(X_train, W)\n","            l_epoch = self.loss(y_train, self.sigmoid(z))\n","            loss_list.append(l_epoch)\n","        \n","        self.model = W\n","        self.losses = loss_list\n","        # return loss_list, W\n","        \n","    def predict(self, X):\n","        X = np.hstack((np.ones((X.shape[0], 1)), X))\n","        z = np.dot(X, self.model)\n","        p = self.sigmoid(z)\n","        mask_true = p >= 0.5\n","        mask_false = p < 0.5\n","        p[mask_true] = 1\n","        p[mask_false] = 0\n","        return p"]},{"cell_type":"markdown","metadata":{"id":"-YKjIFRg1ill"},"source":["## Testear con Datasets sintéticos"]},{"cell_type":"markdown","metadata":{"id":"EY5gBif41ill"},"source":["La librería Scikit-Learn tiene una función make_classification que nos permite armar datasets de prueba para problemas de clasificación. Prueben con datasets que tengan varios clusters por clase, que tengan menor o mayor separación y calculen las métricas en cada caso."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjHJSZeY1iln","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"error","timestamp":1651148966420,"user_tz":180,"elapsed":380,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"}},"outputId":"9c4996b8-2ec4-4e47-a589-4353e2d37365"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-de083f10ce80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_redundant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_informative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters_per_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_samples_generator.py\u001b[0m in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_informative\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_redundant\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_repeated\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         raise ValueError(\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;34m\"Number of informative, redundant and repeated \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;34m\"features must sum to less than the number of total\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;34m\" features\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of informative, redundant and repeated features must sum to less than the number of total features"]}],"source":["from sklearn.datasets import make_classification\n","X, y = make_classification(n_features=2, n_redundant=0, n_informative=3, random_state=1, n_clusters_per_class=3)"]},{"cell_type":"code","source":["print(X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PF1i1TZ6EKTH","executionInfo":{"status":"ok","timestamp":1651147660938,"user_tz":180,"elapsed":282,"user":{"displayName":"Benjamin Tourn","userId":"03226511506067021855"}},"outputId":"7595dfb1-c500-43d7-8166-559358b71a72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(100, 4)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"enGyRqjWEV4F"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"Logistic Regression - NumPy.ipynb","provenance":[{"file_id":"https://github.com/btourn/EIA-IntroIA-Practicas/blob/main/GuiaClase6/Logistic%20Regression%20-%20NumPy.ipynb","timestamp":1651144964483}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}